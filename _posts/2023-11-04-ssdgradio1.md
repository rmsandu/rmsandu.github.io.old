---
layout: post
title: Faster image generation with Distilled Stable Diffusion (Segmind) and Gradio
subtile: What I learned (so far) about stable diffusion
cover-img: /assets/img/warrior_cat.jpg
thumbnail-img: /assets/img/warrior_cat.png
tags: [stable diffusion, genai, gradio]
published: true
---


Stable diffusion is an exciting field in GenAI for generating all sorts of images, videos, but also to enhance your own picture. There are many many flavours of stable diffusion models (fine-tuned for a specific tasks, or one specific training images) and versions (v1, v2, XL, etc.). For this tutorial I will be using [Segmind Stable Diffusion](https://github.com/segmind/distill-sd) (SSD), a 50% smaller and faster stable diffusion that generates pics as good as the OG stable diffusion from [Stability AI](https://stability.ai/stable-diffusion/). Aditionally, I will use the HuggingFace implementation of [SSD](https://huggingface.co/segmind/SSD-1B) in a Pythob Notebook on [Google Colab](https://colab.research.google.com/) and prompt the SSD model plus display the generated the images in [Gradio](https://www.gradio.app/)


## Part 1: Understanding Stable Diffusion - a very high-level short introduction

### 1.1 What is Stable Diffusion?

Stable diffusion refers to the process in which particles or information gradually disperses in a medium while maintaining a steady state. This stable state is achieved when the rate of diffusion is uniform and consistent, preventing the formation of imbalances or irregularities. In the context of machine learning, stable diffusion helps in evenly distributing data points, ensuring that the model provides accurate predictions.

### 1.2 Many versions of stable diffusion there are

Stable Diffusion, a state-of-the-art deep learning text-to-image model, was released in 2022. It employs diffusion techniques to generate detailed images based on text descriptions and can perform tasks like inpainting, outpainting, and image-to-image translations. Developed by the CompVis Group at Ludwig Maximilian University of Munich and Runway, it received support from Stability AI and training data from non-profit organizations. This model is notable for its public release of code and model weights, allowing it to run on consumer hardware with a modest GPU (which is also what we are going to do in this tutorial, using the Google Colab free tier). This approach contrasts with proprietary models like DALL-E and Midjourney, which are cloud-service based ([source 1](https://en.wikipedia.org/wiki/Stable_Diffusion), [source 2](https://en.wikipedia.org/wiki/Stable_Diffusion), [source 3](https://en.wikipedia.org/wiki/Stable_Diffusion)).

The latest version of Stable Diffusion, Version 2.1, released in December 2022, introduced improvements such as a new text encoder (OpenCLIP) developed by LAION, providing a broader range of expression. The model's training involved a more diverse dataset, though it faced challenges in representing people due to dataset filtering. Version 2.1 improved on this by adjusting filters to balance image quality and representation of people and pop culture, offering improved rendering of various art styles ([source 4](https://stability.ai/blog/stable-diffusion-v2-1-and-dreamstudio-updates-7-dec-22), [source 5](https://stability.ai/blog/stable-diffusion-v2-1-and-dreamstudio-updates-7-dec-22)).

DALL-E, developed by OpenAI, is renowned for generating images from text descriptions, leveraging a massive dataset to produce imaginative and unique visuals. It's praised for its creativity, conceptual understanding, and versatility. However, it faces challenges such as dataset bias, significant computational resource requirements, and limited control over image generation ([source 6](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 7](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 8](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output)).

Midjourney, developed by researchers from the University of California, Berkeley, specializes in image manipulation and interpretation. It allows users to modify existing images interactively, retaining the overall context while manipulating specific attributes. While it provides interactive manipulation and intuitive controls, it depends on existing images and faces limitations in handling complex transformations ([source 9](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 10](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 11](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 12](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 13](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 14](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output), [source 15](https://narrationbox.com/dall-e-vs-mid-journey-an-overview-comparison-and-a-case-study-in-prompt-output)).

### 1.3 How to use Stable Diffusion model on your computer

To use Stable Diffusion models, one can download the software and run it on their computer, or use web services like Clipdrop or DreamStudio. Addtionally, one can use one of the many version of stable diffusion deployed on Hugging Face or use a WebUI software like [Automatic1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/master) or [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

### 1.4 Segmind Distilled Stable Diffusion (SSD) - Stable Diffusion Lite

This iteration of Stable Diffusion represents a knowledge-distilled, more compact rendition of its larger counterpart. These distilled models achieve image quality comparable to the full-sized Stable Diffusion model, but with notable improvements in terms of speed and resource efficiency.

**Knowledge-Distillation** training involves a process where a large teacher model imparts its wisdom to a smaller student model, step by step. In this analogy, a substantial teacher model is initially trained on extensive datasets. Subsequently, a smaller model undergoes training on a more modest dataset, with the goal of replicating the outputs of the larger model, in addition to conventional training on the dataset. You can read more on their [Github Page](https://github.com/segmind/distill-sd).

#### Advantages

- Up to 100% Faster Inferences: The distilled models exhibit significantly faster inference times.
- Up to 30% Lower VRAM Footprint: Resource efficiency is enhanced with a reduced VRAM footprint.
- Faster Dreambooth and LoRA Training: Improved speed in Dreambooth and LoRA training processes.

#### Limitations

While promising, it's essential to acknowledge the early stage of the distilled models. The outputs may not yet meet high quality and accuracy standards which I can also confirm after playing with Stability Diffusion XL from Stability-AI as an example and just visually comparing the outputs for the same prompt.


## Part 2: Gradio Interface for Stable Diffusion
[Gradio](https://www.gradio.app/) is an open-source framework that allows users to create **customizable user interfaces for machine learning models.** In this section, we will discuss how to customize Gradio to visualize the outputs of stable diffusion. I personally don't like how the standard gradio interface looks like. Maybe orange is not my color? So I set out to modify the theme color, fonts and add some text and pics using HTML and CSS.

![Gradio Standard Look](/assets/img/gradio.png){: .mx-auto.d-block :}

### 2.1 Installing Gradio and getting the hang of it

Gradio continously updates their app so be careful what version you use. While writing this code they just updated to the version 4.2.0 and my code broke :). Take a look at the latest changes and if they are breaking ones in their [CHANGELOG](https://github.com/gradio-app/gradio/blob/main/CHANGELOG.md).

The tricky part was working with HTML and CSS to add my own custom changes. I fiddled a lot with the right changes and for the life of me I couldn't get the color of the background text to change to white.  

Besides their standard orange theme, Gradio offers a few other in-built [themes](https://www.gradio.app/main/docs/themes) in different colors and different fonts. However, I found the options quite limited. The good part is that you can directly [test into the Gradio](https://www.gradio.app/guides/theming-guide) any changes you make to a theme and see how it looks like.

Many creative people built their own themes, ranging from anime girls to the popular "Dracula" theme, and deployed these on [Hugging Face Spaces](https://huggingface.co/spaces/gradio/theme-gallery). I selected the "JohnSmith9982/small_and_pretty" pretty theme because the green colors went well with my background picture which I added using a small custom CSS code. Both the theme and CSS code can be directly passed to`gradio.Blocks()`. Gradio's Blocks API serves as a low-level interface for crafting highly customizable web applications and demos entirely in Python. It surpasses the `Interface` class from Gradio in flexibility, offering control over component layout, event triggers for function execution, and data flows between inputs and outputs. Blocks also supports the grouping of related demos, such as through tabs.

#### Basic Usage

  1. Create a Blocks object.
  2. Use it as a context with the "with" statement.
  3. Define layouts, components, or events within the Blocks context.
  4. Finally, call the `launch()` method to initiate the demo.

Here is the code snippet that achieves that:

```python
!pip install gradio
theme='JohnSmith9982/small_and_pretty'
css_code = """
.gradio-container {
    background: url('file=/content/drive/My Drive/gradient.jpg');
    background-repeat: no-repeat;
    background-attachment: fixed;
    background-size: cover;
    color: white !important; /* Set text color to white */
}
footer {
    display: none !important;
}
"""

with gr.Blocks(theme=theme, css=css_code) as demo:
      ....
```

And here how is the resulting theme looks like:
![Gradio New Look](/assets/img/gradio_interface_van_gogj.png){: .mx-auto.d-block :}

Besides generating funny pictures of cats (like you can see in the thumbnail), you can generate more artistic pictures, in Van Gogh Style for example. 

![van gogh 1 with sharks](/assets/img/vangogh_genai1.png){: .mx-auto.d-block :}
![van gogh 2 with sharks](/assets/img/vangogh_genai1.png){: .mx-auto.d-block :}


Let's discuss now the parameters you can see exposed in the Gradio UI. But first the Python code:

```python
with gr.Blocks(theme=theme, css=css_code) as demo:
        with gr.Row():
            markdown = gr.Markdown("Choose a Stable Diffusion model and specify the variations you want in your image using prompting.", elem_id="white")
        with gr.Row():
            with gr.Column(scale=1):
                model = gr.Dropdown(["ssd", "fine-tuning"], label='Model', value="ssd", interactive=True, elem_id="dropdown")
                prompt = gr.Textbox(label = 'Prompt', value = "a photo of a mountain landscape in summer, van gogh style, sharks jumping out of water",
                                    placeholder="Describe what you want to generate")
                neg_prompt = gr.Textbox(label = 'Negative Prompt', value="blurry, dark, multiple, duplicates, distorted, cropped, low-quality, ugly, comics, cartoon, drawing, text, logo, watermark, signature, close-up",
                                      placeholder="Unwanted in the generated image")
                num_images = gr.Slider(minimum=1, maximum=10, value=2, step=1, label='Number of Samples per Run', interactive=True)
                guidance = gr.Slider(minimum=0, maximum=20, value=7, step=0.5,
                                            label='Guidance Scale (Typically between 7.5 and 12.5)', interactive=True)
                steps = gr.Slider(minimum=10, maximum=100, value=40, step=10,
                                            label='Number of Steps for Inference', interactive=True)
                seed = gr.Slider(minimum=1, maximum=99999, value=1, step=1,
                                            label='Seed', interactive=True)
            with gr.Column(scale=1):
                output = gr.Gallery(show_share_button = True)
                btn1 = gr.Button(value="Inference")
                with gr.Column(scale=1):
                      None
                with gr.Row():
                    with gr.Column(scale=1):
                      my_gogh = gr.Image("/content/drive/My Drive/vangogh.jpg", label=None, show_label=False, elem_id="output_image",
                            show_download_button=False, container=False, scale=0.5)
                      gr.HTML("""<div id='output_image' style='display:block; margin-left: auto; margin-right: auto;
                      align-items: center; justify-content: center;'></div>""")
        btn1.click(inference, inputs=[model, prompt, neg_prompt, guidance, steps, seed, num_images], outputs=output)

demo.queue()
demo.launch(allowed_paths=["."],  debug=True, share=True, max_threads = 40)
```


